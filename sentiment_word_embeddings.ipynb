{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ID: V01053626\n",
        "\n",
        "Name: Newsha Bahardoost"
      ],
      "metadata": {
        "id": "WbaMUDusu06W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHP1WnSvqipb",
        "outputId": "6dcd6850-1ed0-4852-c3b2-2067837a920e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3645\n",
            "['Secretary', 'manages', 'a', 'neat', 'trick,', 'bundling', 'the', 'flowers', 'of', 'perversity,', 'comedy', 'and', 'romance', 'into', 'a', 'strangely', 'tempting', 'bouquet', 'of', 'a', 'movie.']\n",
            "['The', 'story', 'is', 'familiar', 'from', 'its', 'many', 'predecessors;', 'like', 'them,', 'it', 'eventually', 'culminates', 'in', 'the', 'not-exactly', '-stunning', 'insight', 'that', 'crime', 'doesnt', 'pay.']\n",
            "['(An)', 'absorbing', 'documentary.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Task 1: Tokenization\n",
        "with open('/content/p1-sentiments.txt', 'r') as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "processed_sentences = []\n",
        "all_tokens = []\n",
        "\n",
        "for line in sentences:\n",
        "    line_clean = re.sub(r\"'s\\b\", \"\", line.strip())  # Remove 's suffix\n",
        "    line_clean = re.sub(r\"'\", \"\", line_clean)       # Remove apostrophes\n",
        "    tokens = line_clean.split()\n",
        "    processed_sentences.append(tokens)\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "vocab = list(set(all_tokens))\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "# Print tokenization of 7th and 256th sentences (0-based index)\n",
        "print(processed_sentences[6])\n",
        "print(processed_sentences[255])\n",
        "print(processed_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Word Embeddings\n",
        "# Create word-to-index mappings\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Generate training data (target, context pairs)\n",
        "training_data = []\n",
        "for sent in processed_sentences:\n",
        "    for i in range(len(sent)):\n",
        "        target = sent[i]\n",
        "        start = max(0, i - 2)\n",
        "        end = min(len(sent), i + 3)\n",
        "        context = sent[start:i] + sent[i+1:end]\n",
        "        for ctx in context:\n",
        "            training_data.append((target, ctx))\n",
        "\n",
        "# Convert to indices\n",
        "X = [word2idx[target] for target, _ in training_data]\n",
        "Y = [word2idx[ctx] for _, ctx in training_data]\n",
        "\n",
        "X_tensor = torch.LongTensor(X)\n",
        "Y_tensor = torch.LongTensor(Y)\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the model\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "        model = Word2Vec(vocab_size, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_Y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Function to find closest words\n",
        "def get_closest_words(word, embeddings, word2idx, idx2word, top_k=5):\n",
        "    if word not in word2idx:\n",
        "        return []\n",
        "    idx = word2idx[word]\n",
        "    embed = embeddings[idx]\n",
        "    cos_sims = F.cosine_similarity(embed.unsqueeze(0), embeddings, dim=1)\n",
        "    sorted_indices = torch.argsort(cos_sims, descending=True)\n",
        "    closest = [i.item() for i in sorted_indices if i != idx][:top_k]\n",
        "    return [idx2word[i] for i in closest]\n",
        "\n",
        "embeddings = model.embedding.weight.data\n",
        "words = ['actor', 'actress', 'good', 'bad', 'awesome']\n",
        "\n",
        "# Handle potential typo in 'actress' if needed (assuming it's a typo for 'actress')\n",
        "target_words = []\n",
        "for word in words:\n",
        "    if word in word2idx:\n",
        "        target_words.append(word)\n",
        "    elif word == 'actress' and 'actress' not in word2idx:\n",
        "        # Check for possible typo in the problem statement\n",
        "        pass\n",
        "\n",
        "# Print closest words for each target word\n",
        "for word in ['actor', 'actress', 'good', 'bad', 'awesome']:\n",
        "    closest = get_closest_words(word, embeddings, word2idx, idx2word)\n",
        "    print(f\"{word} => {closest}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-wzQfvbt7kY",
        "outputId": "22ff36e6-6e33-462c-cdf2-08b2f5948ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.712689130805259\n",
            "Epoch 2, Loss: 6.6981749322063235\n",
            "Epoch 3, Loss: 6.690814162409583\n",
            "Epoch 4, Loss: 6.684509566587995\n",
            "Epoch 5, Loss: 6.680273312930913\n",
            "Epoch 6, Loss: 6.6742801629295645\n",
            "Epoch 7, Loss: 6.669197397638661\n",
            "Epoch 8, Loss: 6.664893476537956\n",
            "Epoch 9, Loss: 6.6595533115919245\n",
            "Epoch 10, Loss: 6.6560081834940945\n",
            "Epoch 11, Loss: 6.651330020076545\n",
            "Epoch 12, Loss: 6.647077454152957\n",
            "Epoch 13, Loss: 6.643319750016974\n",
            "Epoch 14, Loss: 6.6395783932634105\n",
            "Epoch 15, Loss: 6.63472150647363\n",
            "Epoch 16, Loss: 6.633213423943335\n",
            "Epoch 17, Loss: 6.628773003585579\n",
            "Epoch 18, Loss: 6.626757219780323\n",
            "Epoch 19, Loss: 6.62289590151735\n",
            "Epoch 20, Loss: 6.6198351734368375\n",
            "Epoch 21, Loss: 6.617476792298546\n",
            "Epoch 22, Loss: 6.615345038184824\n",
            "Epoch 23, Loss: 6.611674904823303\n",
            "Epoch 24, Loss: 6.610792574956435\n",
            "Epoch 25, Loss: 6.607005685798882\n",
            "Epoch 26, Loss: 6.604462513627932\n",
            "Epoch 27, Loss: 6.603265482325886\n",
            "Epoch 28, Loss: 6.60023615341778\n",
            "Epoch 29, Loss: 6.598652739857519\n",
            "Epoch 30, Loss: 6.596086372700772\n",
            "Epoch 31, Loss: 6.594864718673765\n",
            "Epoch 32, Loss: 6.592744754266369\n",
            "Epoch 33, Loss: 6.590854995010435\n",
            "Epoch 34, Loss: 6.589420240054759\n",
            "Epoch 35, Loss: 6.586372099181478\n",
            "Epoch 36, Loss: 6.585652506628702\n",
            "Epoch 37, Loss: 6.582989980084028\n",
            "Epoch 38, Loss: 6.581923565199209\n",
            "Epoch 39, Loss: 6.579217130823653\n",
            "Epoch 40, Loss: 6.577671771825746\n",
            "Epoch 41, Loss: 6.577683622522872\n",
            "Epoch 42, Loss: 6.574087561563004\n",
            "Epoch 43, Loss: 6.574516530184782\n",
            "Epoch 44, Loss: 6.570555975270826\n",
            "Epoch 45, Loss: 6.569192544434422\n",
            "Epoch 46, Loss: 6.568265025929887\n",
            "Epoch 47, Loss: 6.56615272972935\n",
            "Epoch 48, Loss: 6.565896049026371\n",
            "Epoch 49, Loss: 6.563911057257837\n",
            "Epoch 50, Loss: 6.5623635164527006\n",
            "actor => ['surf', 'self-hating,', 'Kwan', 'flick.', 'Jean']\n",
            "actress => ['sometimes', 'sneaks', 'delicious', 'Scream.', 'childish']\n",
            "good => ['gander,', 'suspense.', 'myth.', 'forward', 'acting.']\n",
            "bad => ['disgust,', 'Cimarron', 'proceedings', 'wry', 'chops']\n",
            "awesome => ['smell', 'Diane', 'maybe,', 'essential', 'realize']\n"
          ]
        }
      ]
    }
  ]
}